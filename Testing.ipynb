{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://192.168.56.200:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EventId',\n",
       " 'DER_mass_MMC',\n",
       " 'DER_mass_transverse_met_lep',\n",
       " 'DER_mass_vis',\n",
       " 'DER_pt_h',\n",
       " 'DER_deltaeta_jet_jet',\n",
       " 'DER_mass_jet_jet',\n",
       " 'DER_prodeta_jet_jet',\n",
       " 'DER_deltar_tau_lep',\n",
       " 'DER_pt_tot',\n",
       " 'DER_sum_pt',\n",
       " 'DER_pt_ratio_lep_tau',\n",
       " 'DER_met_phi_centrality',\n",
       " 'DER_lep_eta_centrality',\n",
       " 'PRI_tau_pt',\n",
       " 'PRI_tau_eta',\n",
       " 'PRI_tau_phi',\n",
       " 'PRI_lep_pt',\n",
       " 'PRI_lep_eta',\n",
       " 'PRI_lep_phi',\n",
       " 'PRI_met',\n",
       " 'PRI_met_phi',\n",
       " 'PRI_met_sumet',\n",
       " 'PRI_jet_num',\n",
       " 'PRI_jet_leading_pt',\n",
       " 'PRI_jet_leading_eta',\n",
       " 'PRI_jet_leading_phi',\n",
       " 'PRI_jet_subleading_pt',\n",
       " 'PRI_jet_subleading_eta',\n",
       " 'PRI_jet_subleading_phi',\n",
       " 'PRI_jet_all_pt',\n",
       " 'Weight',\n",
       " 'Label']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = spark.read.format('csv').option('header','true').option('inferSchema', 'true').load('training.csv')\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label_index\").fit(raw_data)\n",
    "train = indexer.transform(raw_data)\n",
    "#train.show()\n",
    "\n",
    "train_clean = train.drop(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=train_clean.columns\n",
    "cols.remove(\"Label_index\")\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "train_df=assembler.transform(train_clean)\n",
    "train_df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "train_df=standardscaler.fit(train_df).transform(train_df)\n",
    "train_df.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size=float(train.select(\"Label_index\").count())\n",
    "numPositives=train.select(\"Label_index\").where('Label_index == 0').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = LogisticRegression().setWeightCol(\"classWeights\").setLabelCol(\"Outcome\").setFeaturesCol(\"Aspect\")\n",
    "lr = LogisticRegression(labelCol=\"Label_index\", featuresCol=\"Scaled_features\",maxIter=10)\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_test.select(\"Label_index\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(model.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()\n",
    "\n",
    "trainingSummary = model.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Label_index\")\n",
    "\n",
    "\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = spark.read.format('csv').option('header','true').option('inferSchema', 'true').load('test.csv')\n",
    "# test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tindexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label_index\").fit(test_data)\n",
    "# test = tindexer.transform(test_data)\n",
    "# test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcols=test_data.columns\n",
    "# # tcols.remove(\"Label\")\n",
    "# tassembler = VectorAssembler(inputCols=tcols,outputCol=\"features\")\n",
    "# # Now let us use the transform method to transform our dataset\n",
    "# test_df=tassembler.transform(test_data)\n",
    "# test_df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "# test_df=standardscaler.fit(test_df).transform(test_df)\n",
    "# test_df.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# evaluator = BinaryClassificationEvaluator()\n",
    "# print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=train.columns\n",
    "# cols.remove(\"Label\")\n",
    "# train_clean = train.drop(\"Label\")\n",
    "# train_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
